In Chapter 3, you learned that critics use LLMs as judges. But here's an uncomfortable truth: LLM judges are biased. Every major language model has systematic evaluation tendencies that can distort your scores.

Understanding and mitigating these biases is essential for trustworthy evaluation results.

## The Bias Problem

Research on LLM-as-judge systems has identified several persistent bias patterns:

### Position Bias

LLMs tend to favor content that appears in certain positions within a prompt. In pairwise comparisons, some models prefer the first option; others prefer the second. In scoring tasks, the order in which evaluation criteria are presented can influence scores.

### Verbosity Bias

Longer, more elaborate content tends to receive higher scores, regardless of actual quality. A verbose but off-brand response may score higher than a concise but perfectly in-character one.

### Self-Preference Bias

LLMs tend to rate content generated by the same model more favorably. If you use GPT-4 to both generate character dialogue and evaluate it, the evaluator may give systematically higher scores than a neutral human judge would.

### Anchoring Effects

When a critic's prompt includes examples or reference scores, the model may anchor to those values rather than independently assessing the content.

## Multi-Provider Execution

The most effective bias mitigation strategy is to use multiple LLM providers as judges and aggregate their scores. If GPT-4 and Claude evaluate the same content independently, their individual biases tend to cancel out.

CanonSafe implements this through the `call_both_llms_json()` function in the LLM adapter layer. When multi-provider evaluation is enabled, each critic's evaluation runs against both providers in parallel:

1. **OpenAI** (GPT-4) receives the evaluation prompt and returns a score + reasoning
2. **Anthropic** (Claude) receives the same prompt and returns its own score + reasoning

The system then:

- **Averages** the two scores for the final critic score
- **Combines** the reasoning from both models into a merged explanation
- **Flags disagreement** when the score delta exceeds 0.3 (configurable threshold)

### Why 0.3?

A delta of 0.3 on a 0-1 scale represents a meaningful disagreement. If one model says "0.85" (pass) and the other says "0.50" (quarantine), those are fundamentally different assessments. The system flags this as `judge_disagreement`, which can:

- Automatically route the evaluation to human review
- Be tracked as a metric over time (high disagreement rates suggest prompt issues)
- Trigger investigation into which model's assessment is more accurate

## Inter-Critic Agreement

Beyond multi-provider agreement for a single critic, CanonSafe also measures **inter-critic agreement** — how much different critics agree about the same piece of content.

The formula is straightforward: calculate the standard deviation of all critic scores for an evaluation run. Low standard deviation means critics agree; high standard deviation means they disagree.

| Standard Deviation | Interpretation |
|-------------------|----------------|
| σ < 0.1 | Strong agreement — critics see the same quality level |
| 0.1 ≤ σ < 0.2 | Normal variation — different dimensions naturally score differently |
| 0.2 ≤ σ < 0.3 | Moderate disagreement — worth investigating |
| σ ≥ 0.3 | High disagreement — flag for review |

High inter-critic disagreement often means one of:

- **The content is genuinely mixed** — strong on some dimensions, weak on others (e.g., perfect voice but wrong canon facts)
- **A critic's rubric needs calibration** — one critic is systematically scoring too high or too low
- **The content is ambiguous** — reasonable judges could disagree, which is exactly when human review adds value

## Building Trustworthy Scores

Combining multi-provider execution with inter-critic agreement monitoring creates a two-layer reliability system:

1. **Within each critic**: Multiple LLM providers reduce individual model bias
2. **Across critics**: Agreement monitoring catches when the evaluation system is uncertain

Neither layer alone is sufficient. Multi-provider execution doesn't help if your prompt template is fundamentally flawed (both models will give wrong answers). Inter-critic agreement doesn't catch cases where all critics share a common bias (e.g., all rating verbosity too highly).

Together, they provide a reasonable level of confidence that evaluation scores reflect actual content quality.

---

## Hands-On: Judge Reliability in Practice

### The Judge Registry

Navigate to the **Judge Registry** page (under Configuration in the sidebar). This page shows all registered judge models — both LLM providers and any custom judges you've configured.

Each judge entry shows:

- Model name and provider
- Health status (is the API accessible?)
- Average response time
- Total evaluations performed

[SCREENSHOT: Judge Registry page showing registered models with health indicators]

### Examining Multi-Provider Results

Navigate to **Evaluations** and click into a completed evaluation result. If multi-provider evaluation was enabled, you'll see score contributions from both providers in the critic detail.

Look for the `judge_disagreement` flag in the evaluation metadata. On evaluations where both providers were close (delta < 0.3), the flag is absent. On evaluations where they diverged significantly, the flag is present and the combined reasoning explains both perspectives.

[SCREENSHOT: Evaluation result detail showing individual provider scores and agreement status]

### Inter-Critic Agreement Score

On the same evaluation result, look at the inter-critic agreement section. This shows the standard deviation across all critic scores and whether the agreement level is normal or flagged.

An evaluation where Canon Fidelity scored 0.92, Voice Consistency scored 0.88, and Safety scored 0.90 has high agreement (σ ≈ 0.02). An evaluation where Canon Fidelity scored 0.95 but Safety scored 0.45 has a significant disagreement (σ ≈ 0.25) — worth investigating why safety scored so differently from canon.

[SCREENSHOT: Evaluation result showing inter-critic agreement score with standard deviation]

### When Judges Disagree

Judge disagreement isn't necessarily bad — it's informative. When you see disagreement flags:

1. **Read the reasoning from both providers** — often the disagreement reveals a genuine ambiguity in the content
2. **Check the critic's prompt template** — vague evaluation instructions produce inconsistent results
3. **Consider the content** — borderline content naturally produces more disagreement than clearly good or clearly bad content

Over time, tracking disagreement rates by critic helps you identify which critics are well-calibrated and which need prompt refinement.

> **Key Takeaway**: LLM judges have systematic biases (position, verbosity, self-preference, anchoring). Multi-provider execution (running the same evaluation on both OpenAI and Anthropic) mitigates individual model bias through score averaging and disagreement detection. Inter-critic agreement monitoring adds a second layer by flagging when critics diverge significantly. Together, these mechanisms build trustworthy, auditable evaluation scores.
